# Week 1

1. Distributed Computing:
   - A number of autonomous processing elements (not necessarily homogeneous) that are interconnected by a computer network and that cooperate in performing their assigned tasks.
   - Processing logic, Function, Data, Control
2. Distributed Database:
   - A distributed database (DDB) is a collection of multiple, logically interrelated databases distributed over a computer network.
   - A distributed database management system (D–DBMS) is the software that manages the DDB and provides an access mechanism that makes this distribution **transparent** to the users.
3. Twelve Commandments for Distributed Databases:
   - <u>Local site</u> independence
   - <u>Central site</u> independence
   - <u>Failure</u> independence
   - <u>Location</u> transparency
   - <u>Fragmentation</u> transparency
   - <u>Replication</u> transparency
   - <u>Distributed query processing</u>
   - <u>Distributed transaction processing</u>
   - <u>Hardware</u> independence
   - <u>Operating system</u> independence
   - <u>Network</u> independence
   - <u>Database</u> independence








# Week 2

1. Fragmentation:

   {0}. Horizontal Fragmentation (HF)
      {0}. Primary Horizontal Fragmentation (PHF)
      {0}. Derived Horizontal Fragementation (DHF)
   {0}. Vertical Fragmentation (VF)
   {0}. Hybrid Fragmentation (HF)

2. Four Categories of information requirement:

   {0}. Database information
   {0}. Application information
   {0}. Communication network information
   {0}. Computer system information

3. **Primary horizontal fragmentation** 

   {0}. Defined by <u>**a selection operation** on the owner relations of a database schema</u>. 
   {0}. A horizontal fragment Ri of relation R consists of all the tuples of R which satisfy a minterm predicate mi.

4. Completeness:

   {0}. A set of simple predicates Pr is said to be complete if and only if the accesses to the tuples of the minterm fragments defined on Pr requires that **two tuples of the same minterm fragment have the same probability of being accessed by any application**.

5. Minimality:

   {0}. The simple predicate should be **relevant** in determining a fragmentation
   {0}. If all the predicates of a set Pr are relevant, then Pr is **minimal**

6. Disjointness:

   {0}. Minterm predicates that form the basis of fragmentation should be **mutually exclusive**

7. Allocation Alternatives:

   {0}. **Non-replicated** database containts fragments that are allocated to sites, with each fragment residing at **only one site** and there is only one compy of any fragment on the network.

   - **Replication**: either each fragment exists at each site (**fully replicated** database), or each fragment is distributed to some of the sites (**partially replicated** database)

8. Derived Horizontal Fragmentation: 

  {0}. Defined on a member relation of a link according to a selection operation specified on its owner.
  {0}. A和B表有关联，使用B表的某个属性，作为A表的分割依据

9. Vertical Fragmentation:

   {0}. **Grouping**: start by assigning each attribute to one fragment, and at each step, joins some of the fragemnts untial some criteria is satisfied 
   {0}. **Splitting**: start with a relation and decides on beneficial partitionings based on the access behaviour of applications to the attributes
   {0}. Application Information:
      {0}. Attribute affinities:
         {0}. a measure that indicates how closely related the attributes are
         {0}. This is obtained from more primitive usage data








# Week 3

1. Query processer:
   {0}. A distributed query processer: maps high level query into a sequence of database operations on fragments.
2. Main operations in the query:
   {0}. SELECT, JOIN, PROJECT, UNIT, INTERSECTION, SET DIFFERENCE, CARTESIAN PRODUCT
3. Nature Join, equijoin and Semijoin:
   {0}. Nature join is same as equijoin, however the result of a nature join have attributes from both relations but the common attribute appears only once, but the result of equijoin will have more than one common attributes 
   {0}. A semijoin returns rows from the first table when at least one match exists in the second table.
4. Query Optimization Issues: Types Of Optimizers:
   - Exhaustive search
     - Cost-based
     - Optimal
     - Combinatorial complexity in the number of relations
   - Heuristics
     - Not optimal
     - Regroup common sub-expressions
     - Perform selection, projection first
     - Replace a join by a series of semijoins
     - Reorder operations to reduce intermediate relation size
     - Optimize individual operations
5. Optimization Timing:
   {0}. Static: 
      {0}. compilation: optimize **prior to the execution**
      {0}. difficult to estimate the size of the intermediate results: error propagation
      {0}. can amortize over many executions
   {0}. Dynamic:
      {0}. **run time optimization**
      {0}. exact information on the intermediate relation sizes
      {0}. must be repeated and re-optimize for multiple executions
   {0}. Hybrid:
      {0}. compile using a static algorithm
      {0}. if the error in estimate sizes, reoptimize at run time
6. Decision sites:
   {0}. Centralized:
      {0}. single site determines the best schedule
      {0}. it is simple
      {0}. need knowledge about the entire distributed database
   {0}. Distributed:
      {0}. cooperation among sites to determine the schedule
      {0}. need only local information
      {0}. cost of cooperation
   {0}. Hybrid:
      {0}. one site determines the global schedule
      {0}. each site optimizes the local subqueries
7. Distributed Query Processing methodology:
   {0}. Step 1 Query Decomposiiton:
      {0}. normalization
         {0}. manipulate query quantifiers and qualification
      {0}. analysis
         {0}. detect and reject incorrect queries
            {0}.  Type incorrect
               {0}. If any of its attribute or relation names are not defined in theglobal schema
               {0}. If operations are applied to attributes of the wrong type
            {0}. Semantically incorrect
               {0}. Components do not contribute in any way to the generation of theresult
         {0}. possible for only a subset of relational calculus
      {0}. simplification
         {0}. eliminate redundant predicates
      {0}. restructuring
         {0}. calculus query —> algebraic query
         {0}. more than one translation is possible 
         {0}. use transformation rules
   {0}. Step 2 Data localization:
      {0}. determine which fragments are involved 
      {0}. localization program
         {0}. substitute for each global query its materialization program
         {0}. optimize








# Week 4

1. Transaction:
   - A transaction is a collection of actions that make consistent transfromations of systems states while preserving system consistency
2. ACID principles:
   - Atomicity:
     - Either all of none of the transaction's operations are performed
     - Atomicity requires that if a transaction is interrupted by a failure, its partial reuls must be undone
     - The activity of preserving the transaction's atomicity in presence of transaction aborts due to input errors, system overloads, or deadlocks is called transaction recovery
     - The activity of ensuring atomicity in the presence of system crashes is called crash recovery
   - Consistency:
     - The consistency of a transaction is simply its correctness.
     - Internal consistency:
       - A transaction which executes alone against a consistent database leaves it in a conssitent state.
       - Transactions do not violate database integrity constraints.
     - Transactions are correct programs.
     - Database concurrency:
       - **Lost updates**:
         - Lost updates occur when two or more transactions select the same data item and then update the item based on the value originally selected
         - Each transaction is unaware of other transactions. The last update overwrites updates made by the other transactions, which results in lost data
       - **Dirty Read** (Reading uncommited changes): 
         - It occurs when a second transaction selects a row that is being updated by another transaction
         - The second transaction is reading data that has not been committed yet and may be changed by the transaction updating the row
       - **Fuzzy Read** (Unrepeateable Reads):
         - It occurs when a second transaction accesses the same data item several times and reads different data each time.
         - It is similar to dirty read in that another transaction is changing the data that a second transactionis reading.
           - However, in fuzzy read, the data read by the second transaction was committed by the transaction that made the change.
           - Also, it involves multiple reads (two or more) of the same row and each time the information is changed by another transaction.
   - Isolation
     - Serializability: if several transactions are executed concurrently, the results must be the same as if they were executed serially in some order.
     - Incomplete results:
       - An incomplete transaction cannot reveal its results to other transactions before its commitment
       - Necessary to avoid cascading aborts
   - Durability
     - Once a transaction commits, the system must guarantee that the results of its operations will never be lost, in spite of subsequent failures







# Week 5

1. Execution History (Schedule):
   - An order in which the operations of a set of transactions are executed.
   - A history (schedule) can be defined as a partial order over the opeartions of a set of transactions.
2. Serial History:
   - If each transaction is consistent (obeys integrity rules), then the database is guaranteed to be consistent at the end of executing a serial history.
   - Conflicting operations: two incompatible operations (e.g. read and write) conflict if they both access the same data item.
   - Conflict equivalence: the relative order of execution of the conflicting operations belonging to unaborted transactions in two histories are the same.
3. Concurrency Control Algorithms:
   - Pessimistic:
     - This algorithms synchronize the concurrent execution of transactions early in their execution life cycle.
     - Two-Phase Locking-based (2PL)
     - Timestamp Ordering (TO)
     - Hybrid
   - Optimistic:
     - This algorithms delay synchronization of transactions until their termination.
     - Locking-based
     - Timestamp ordering-based
4. Lock granularity:
   - Database-level lock: entire database is locked
   - Table-level lock: entire table is locked
   - Page-level lock: entire diskpage is locked
   - Row-level lock: allow concurrent transactions to access different rows of same table, even if rows are located on same page.
   - Field-level lock: allows concurrent transactions to access same row, as long as they require use of different fields within that row.
5. Two-Phase Locking (2PL):
   - A transacntion locks an object before using it.
   - When an object is locked by another transaction, the requesting transaction must wait.
   - When a transaction releases a lock, it may not request another lock.
   - Each transaction has a **growing phase**, where it obtains locks and access data items, and a **shrinking phase**, during which it releases locks. The **lock point** is the moment when the tranactions has achieved all its locks but has not yet start to releas any of them.
   - In strict 2PL, it hold locks until the end and releases all the locks together when the transaction terminates. 
6. Centralized 2PL:
   - Coordinating TM send Lock Request to Central Site LM
   - Central Site LM send "Lock Granted" to Coordinating TM and then TM pass operation to participating sites
   - When the operation ends, the participating sites send End of Operation to TM
   - TM then "Release Locks" to LM
7. Distributed 2PL Execution:
   - The request is sent to the lock managers at all participating sites.
   - The operation is passes to the data processors by the participating LMs
   - The participating DPs send the "EoO" to the coordinating TM, or DP can send it to its own LM who can release the lock and inform the coordinating TM
8. Timestamp Ordering:
   - Basic Timestamp Ordering:
     - If the new operation belongs to a transaction that is younger than all the confliciting ones that have already been scheduled, the operation is accepted.
     - Otherwise, it is rejected, causing the entire transaction to restart with a new timestamp.
   - Conservative Timestamp Ordering:
     - It delays each operation until there is an assurance that it will not be restarted.
   - Multiversion Timestamp Ordering:
     - It does not modify the values in the database but create new values.
9. Deadlock:
   - A transaction is deadlocked if it is blocked and will remain blocked until there is intervention.
   - Wait-for-graph (WFG): If transaction Ti waits for another transaction Tj to release a lock on an entity, then Ti → Tj in WFG.
10. Deadlock Management:
    - Ignore
      - Let the application programmer deal with it, or restart the system
    - Prevention
      - Guaranteeing that deadlocks can never occur in the first place. Check transaction when it is initiated. Requires no run time support.  
    - Avoidance
      - Deadlock avoidance schemes either employ concurrency control techniques that will never result in deadlocks; or
      - Require that potential deadlock situations are detected in advance and steps are taken such that they will not occur
      - Requires run time support.  
    - Detection and Recovery
      - Allowing deadlocks to form and then finding and breaking them. As in the avoidance scheme, this requires run time support.





# Week 6

1. Types of Failures:
   - Transaction failures
     - Transaction aborts (unilaterally or due to deadlock)
     - Avg. 3% of transactions abort abnormally
   - System (site) failures
     - Failure of processor, main memory, power supply, ...
     - Main memory contents are lost, but secondary storage contents are safe
     - Partial vs. total failure
   - Media failures
     - Failure of secondary storage devices such that the stored data is lost
     - Head crash/controller failure 
   - Communication failures
     - Lost/undeliverable messages
     - Network partitioning
2. Logging:
   - The log contains information used by the recovery process to restore the consistency of a system. This information may include
     - transaction identifier
     - type of operation (action)
     - items accessed by the transaction to perform the action
     - old value (state) of item (before image)
     - new value (state) of item (after image)
3. REDO:
   - REDO'ing an action means performing it again.
   - The REDO operation uses the log information and performs the action that might have been done before, or not done due to failures.  
   - The REDO operation generates the new image.
4. UNDO:
   - UNDO'ing an action means to restore the object to its before image.
   - The UNDO operation uses the log information and restores the old value of the object.
5. Write-Ahead Log Protocol:
   -  Notice:
     - If a system crashes before a transaction is committed, then all the operations must be undone. Only need the before images (undoportion of the log).
     - Once a transaction is committed, some of its actions might have to be redone. Need the after images (redo portion of the log).
   - WAL protocol :
     - Before a stable database is updated, the undo portion of the log should be written to the stable log
     - When a transaction commits, the redo portion of the log must bewritten to stable log prior to the updating of the stabledatabase.
6. Two-Phase Commit (2PL):
   - Phase 1: The coordinator gets the participants ready tow rite the results into the database
   - Phase 2 : Everybody writes the results into the database  
     - Coordinator :The process at the site where the transaction originates and which controls the execution
     - Participant :The process at the other sites that participate inexecuting the transaction
7. Database Replication Design
   - Fully replicated database:
     - Stores multiple copies of each database fragment at multiple sites
   - Partially replicated database:
     - Stores multiple copies of some database fragments at multiple sites
8. Eager/Synchronous Update:
   - All the replicas will have the same value at the end of the update transaction
   - Advantages:
     - All copies identical (mutually consistent)
     - Ensures up-to-date values
   - Disadvantages:
     - Increases response time because thetransaction has to update all copies
     - If one copy is unavailable, the transaction cannot terminate
9. Lazy/Asynchronous Update:
   - The transaction does not wait until its update is written to all replicas before it commits
   - It commits as soon as first copy is updated and then changes are propagated
   - Advantages
   - Reduces response time
   - Disadvantages
   - While the propagation takes place, the copies are mutually inconsistent (stale data)
10. Where update occurs:
    - Centralised Update / Primary Copy approach:
      - All updates occur at just one primary copy (master copy) and then reflected on other copies
      - Read can access any replica 
      - Advantage
        - It is easy as updates happen at the master site
      - Disadvantage
        - Overloading and bottleneck at the master site
        - Single point of failure
    - Distributed Update / Update Everywhere:
      - The update transaction can occur at any of the copies (at any site) and then the updates are propagated to other sites
      - Advantage
        - No bottleneck
        - Better fault tolerance
        - Load is distributed
      - Disadvantage
        - Needs synchronization of copies if different copies of a data item is updated at multiple sites concurrently (when coupled with the lazy method)
11. Replication Strategies:
    - Eager Centralized
      - The updates are first applied to the master/primary copy and then propagated to the slaves synchronously before the transaction commits
    - Eager Distributed
      - An update can originate at any site, applied on the local copy and then propagated to other replicas synchronously before the transaction commits
        - If the update originates at site that data item does not exist,it will be forwarded to one of the sites with a replica
    - <u>Higher consistencies but longer response time</u>
    - Lazy Centralized
      - The updates are first applied to the master/primary copy and then propagated to the slaves asynchronously
      - The propagation happens after the transaction commits
    - Lazy Distributed
      - The updates can occur on the local copy and then propagated asynchronously to the other replicas
      - The propagation happens after the transaction commits
      - It is the most complicated model because multiple transactions can update different copies of the same data item concurrently at different sites (Reconciliation protocols)
    - Shorter response time but inconsistent reads









# Week 7

1. The nature of big data:
   - More data
   - Messy data 
   - Good enough
2. Big data: 
   - Big Data is a term that describes large volumes of high velocity, complex and variable data that require advanced techniques and technologies to enable the capture, storage, distribution, management, and analysis of the information
3. Four V's of Big Data:
   - Volumn: 
     - Data at rest
     - Terabytes to exabytes of exisitng data to process
   - Velocity:
     - Data in motion
     - Streaming data, milliseconds to senconds to respond
   - Variety:
     - Data in many forms
     - Structured, unstructured, text and multimedia
   - Veracity:
     - Data in doubt
     - Uncertainty due to data inconsistency and incompleteness, ambiguities, latency, deception and model approximations
   - Other Vs: Value, Visualisation
4. Data processing and scalability:
   - Vertical scaling (scale up)
     - High performance computer systems
       - Expensive to build, operate and maintain
       - Increase the computation power by increasing the number of CPUs within a single physical machine
   - Horizontal scaling (scale out)
     - A cluster of inexpensive commodity computers
       - Increase the computation power by adding more machines
       - Difficult to maintain ACID principles
       - Cloud computing
5. Brewer's BASE:
   - **B**asically **A**vailable
     - Ensures data availability during partial failures (in a partitioned database)
   - **S**oft-state  
     - the data can be in a changing and inconsistent state
   - **E**ventually Consistent
     - Data eventually becomes consistent
     - Optimistic: “if no new updates are made to the object, eventually all accesses will return the last updated value”
6. CAP theorem:
   - Consistency (**C**): “each server returns the right response to each request”
   - Availability (**A**): “each request eventually receive a response”
   - Partition tolerance (**P**): Operations will continue even though some components are not available (during network partitions)
7. CAP (2 of 3):
   - CA (consistent and available): not partition tolerance
   - AP (available and partition tolerance): not consistent
   - CP (consistent and partition tolerance): not highly available
8. PACELC:
   - If there is a partition (P) how does the systems trade-off availability and consistency (A and C); else (E), when the system is running normally in the absence of partitions, how does the systems trade-off latency(L) and consistency (C)?
   - Tradeoffs:
     - Fully ACID systems insist on maintaining consistency, at the expense of the availability and latency
     - Non-traditional databases give up consistency for availability, and under normal operations they give up consistency for lower latency
9. Big Data Challenges
   - Performance and speed
   - Efficiency
   - Data quality and veracity
   - Data ownership and privacy
   - Data pre-processing and cleaning
   - Data processing and analytics (batch, streaming and hybrid)
   - Visualization
   - Choosing the right platform/tool for your data
   - Transaction management and Tradeoffs (BASE, CAP, PACELC)











# Week 8

1. Pig Latin's Data Model:
   - An atom contains a simple atomic value such as a string or a number, e.g. 'alice' (i.e. a field)
   - A tuple is an ordered set of fields, each of which can be any of the data types, e.g. ('alice','lakers')
   - A bag is a collection of tuples with possible duplicates
     - Tuples in a bag don't need to have the same number and type of fields, or even nested tuples
   - A map is a set of key value pairs  E.g. [name#John,phone#5551212]
2. Hive's Data Model:
   - Tables:
     - Hive stores data in tables
     - Each table consists of a number of rows and columns
     - Each column has an associated type (primitive or complex)
       - complex types such as maps (associative arrays), lists and structs
     - A table is stored in a directory in HDFS
   - Partitions:
     - A table may be partitioned or non-partitioned based on values of one of more columns to prune directories during queries
     - A partition is stored in a subdirectory within a table's directory
     - Partitions enable the user to efficiently identify the rows that satisfy a certain criteria
     - If a table created using the PARTITIONED BY clause, a query can do partition pruning and scan only a fraction of the table relevant to the partitions specified by the query
   - Buckets:
     - Bucketing provides another efficient technique to cluster large sets of data
     - With sampling, we can try out queries on a fraction of data for testing and debugging purpose
     - Unlike partitions, field values are stored in buckets based on a hash function









# Week 9

1. MapReduce:
   - Suitable for those jobs that can be performed in parallel rather than in a serial order
     - Operations such as sum, max, or average
     - Applications in recommendation systems, fraud detection, retail, marketing, web search, and many others
   - More efficient for processing very large datasets 
   - It requires translating the problem in hand to the Map and Reduce computation tasks
   - Multiple maps and reduces can be used for a job
2. Mappers:
   - First spilt the big job (file) into chunks and give each Mapper some chunks (part of the file)
   - Use some type of indexing to identify each store location
   - Pile up results based on the index 
   - Each pile has the pair values of the location and amount per location
   - After mappers finish their jobs, reducers can start their job
3. Reducers:
   - each reducer will be assigned to a store
   - They will fetch the indexed piles from multiple mappers
   - They combine and sort all the different piles based on the location and calculate the final total
4. MapReduce Phases:
   - MapReduce programs are executed actually in 3 stages:
     - Map-> shuffle and sort-> Reduce
   - Mappers are programs that each deal with a relatively small amount of data and work in parallel
   - The output of mapping is called intermediate results/ records in the form of (key, value)
   - Between mapping and reducing there is another step called Shuffle and Sort
     - Shuffle is the transferring the intermediate results from mappers tothe reducers
     - Sort is the step to organise the sets of records into sorted order
   - The final results can be computed, either by having one reducer or by an extra step to merge the results
   - Clients write only the map and reduce functions, and specify input and output locations
   - Both works based on key-value format
   - MapReduce Framework will take care of how to split jobs, load balancing, data distribution, fault tolerance, networking, and synchronization
   - Multiple Map and Reduce functions can run in parallel and independently
   - Reducers can’t start till mappers finish
5. MapReduce Execution Overview:
   - fork
   - assign Map and Reducers
   - Read
   - local Write
   - Remote Read
   - Write
6. Locality:
   - To reduce network bandwidth and data transmission costs:
   - The master considers the location information of the input files before scheduling a map task 
     - It will allocate the task to a machine that contains a replica of the corresponding input data
     - If not possible, it will schedule the map task on a machine near a replica
   - As a result, large amount of data can be read locally
7. Task Granularity:
   - The number of Mappers M and Reducers R should be ideally larger than the number of worker machines
   - The number of M is usually selected such that each individual task is about 64 MB of input data
   - The number of R is usually selected as a smaller number, i.e.a small multiple of the number of worker machines
8. Backup Tasks:
   - When a MapReduce operation is close to completion, the master schedules backup executions of the remaining in-progress tasks
   - The task is considered as completed whenever either the primary or the backup execution completes
   - It increases the computational workload resources but not more than a few percent
   - Significant improvement in reducing the time to complete large MapReduce operations
9. Hadoop Framework:
   - Hadoop Distributed File System (HDFS)
   - Hadoop MapReduce
   - Hadoop YARN
   - Hadoop Common
10. HDFS:
    - HDFS is Hadoop’s distributed file system 
    - A Master/Slave architecture
    - A file is split into one or more blocks (64-128MB)
      - The block size and replication factor are configurable
      - All blocks in a file except the last block have the same size
    - Operations to create, read, write and delete
    - Transparency of physical location of data and replicas
    - A single NameNode and multiple DataNodes
    - NameNode and DataNodes are software pieces running on commodity machines
11. NameNode:
    - NameNode is a dedicated and master server that processesrequests simultaneously from multiple clients
    - All the read and write requests first go to the NameNode
    - It manages the file system and maintains the namespace tree
    - It executes file system namespace operations like opening, deleting, and renaming files and directories
    - It determines the mapping of file blocks to DataNodes (physicallocation of files)
    - It monitors the DataNodes
12. DataNodes:
    - DataNodes store data, one DataNode per node in the cluster
    - They serve read and write requests from the file system’s clients
    - They can create, delete, and replicate blocks based on the NameNode’s instructions
    - DataNodes send heartbeats to the NameNode (monitoring the state)
    - If DataNode fails, NameNode schedules creation of new replicas of those blocks on other DataNodes
13. JobTracker and TaskTracker:
    - JobTracker
      - Schedules MapReduce tasks to nodes in the cluster
      - There is also usually only one JobTracker per cluster
      - It first looks at empty slots on the same server hosting the DataNode containing the data
    - TaskTracker
      - A node in the cluster that accepts tasks (Map, Reduce and Shuffle) from a JobTracker 
      - When the process finishes, the TaskTracker notifies the JobTracker
      - It sends heartbeat messages periodically to the JobTracker
    - One master node acts as the NameNode and JobTracker and the slave nodes act as the DataNodes and TaskTrackers
14. YARN, Yet Antoher Resource Negotiator
    - YARN enables separating resource management and scheduling from the data processing and application management
15. TEZ:
    - Tez is an application framework built on Hadoop Yarn that enables definition of complex data flow pipelines using simple graph connection API’s
    - A more expressive directed acyclic graph (DAG) of tasks, within a single application or job
      - Tez uses DAG execution representing the query instead of multiple stages of MapReduce program which involves a lot of synchronization and IO overheads
      - Tez writes intermediate data set into memory instead of hard disk

















